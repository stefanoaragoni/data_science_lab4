{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Laboratorio 4 - Data Science**\n",
    "### *Mejorando el Análisis de Sentimientos con LSTM y Características Adicionales*\n",
    "Stefano Aragoni, Carol Arévalo\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Importación de Datos`\n",
    "- Utilice el conjunto de datos IMDB proporcionado por Keras. pero esta vez, en lugar de utilizar sólo las 20.000 palabras más frecuentes,utilice las 50.000 palabras más frecuentes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como primer paso, se importan las diferentes librerías a utilizar. Principalmente se queriere de Keras, ya que el conjunto de datos IMBD se encuentra en esta librería. Además, se importan las librerías de numpy y pandas para el manejo de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, se importa el conjunto de datos con 50,000 palabras más frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando los datos...\n",
      "Datos cargados.\n"
     ]
    }
   ],
   "source": [
    "print('Cargando los datos...')\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=50000)\n",
    "\n",
    "X_combined = list(X_train) + list(X_test)\n",
    "y_combined = list(y_train) + list(y_test)\n",
    "df = pd.DataFrame({'Text': X_combined, 'Label': y_combined})\n",
    "\n",
    "print(\"Datos cargados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se optó por mezclar los datos de entrenamiento y prueba, para luego separarlos en 80% y 20% respectivamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, ...      1\n",
       "1  [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463,...      0\n",
       "2  [1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5...      0\n",
       "3  [1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 1...      1\n",
       "4  [1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 1...      0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más específicamente, como se puede observar a continuación, el dataset actual tiene los 50,000 datos. Por tal razón, **se logró la correcta importación del conjunto de datos con 50,000 palabras más frecuentes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de los datos:  (50000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamaño de los datos: \", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con eso listo, se quiso convertir los datos a un formato más legible, por lo que se utilizó **word index** para convertir los datos a texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this film was just brilliant casting location ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>big hair big boobs bad music and a giant safet...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this has to be one of the worst films of the 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the scots excel at storytelling the traditiona...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worst mistake of my life br br i picked this m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  this film was just brilliant casting location ...      1\n",
       "1  big hair big boobs bad music and a giant safet...      0\n",
       "2  this has to be one of the worst films of the 1...      0\n",
       "3  the scots excel at storytelling the traditiona...      1\n",
       "4  worst mistake of my life br br i picked this m...      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = {index: word for word, index in word_index.items()}\n",
    "\n",
    "def decode_sequence(sequence):\n",
    "    decoded_words = [reverse_word_index.get(index - 3, '') for index in sequence]\n",
    "    decoded_words = [word for word in decoded_words if word != '']\n",
    "    return ' '.join(decoded_words)\n",
    "\n",
    "\n",
    "decoded = [decode_sequence(seq) for seq in df['Text']]\n",
    "decoded_df = pd.DataFrame({'Text': decoded, 'Label': df['Label']})\n",
    "\n",
    "decoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Pre-procesamiento`\n",
    "- Secuencie y rellene las críticas para que todas tengan una longitud uniforme.\n",
    "- De las críticas, extraiga características (features) adicionales, por ejemplo. la longitud de la crítica, la proporción de palabras positivas/negativas y cualquier otra que considere pueda ser útil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como primer paso, se limpió el dataset para eliminar stopwords (palabras vacías o irrelevantes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/stefanoaragoni/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Eliminar palabras vacías (stop words)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "decoded_df[\"Text\"] = decoded_df[\"Text\"].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "\n",
    "# Realizar lematización o stemming (opcional)\n",
    "stemmer = PorterStemmer()\n",
    "decoded_df[\"Text\"] = decoded_df[\"Text\"].apply(lambda x: ' '.join(stemmer.stem(word) for word in x.split()))\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "decoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, se procedió a tokenizar los datos, es decir, convertir las palabras a números. Esto se hizo con el fin de poder utilizar los datos en el modelo de LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, se separaron los datos en el conjunto de entrenamiento y prueba. En este caso, se utilizó el 80% de los datos para entrenamiento y el 20% para prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(decoded_df['Text'], decoded_df['Label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, se secuenció y rellenó los diferentes comentarios para que todos tengan una longitud uniforme. En este caso se utilizó una longitud 200. \n",
    "\n",
    "sequence.pad_sequences es una función de Keras que se utiliza para rellenar las secuencias. Si se pasa del largo máximo, se trunca la secuencia. Si es menor, se rellena con ceros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen = 200)\n",
    "X_test = pad_sequences(X_test, maxlen = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Modelo`\n",
    "- Cree un modelo LSTM que acepte las características (features) adicionales junto con la secuencia de palabras.\n",
    "- Intente usar una arquitectura más compleja, incorporando más capas LSTM, capas de Dropout para la regularización y tal vez alguna capa densamente conectada después de la LSTM. (ver también la referencia al final de este documento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Entrenamiento y Evaluación`\n",
    "- Entrene su modelo con el conjunto de datos de entrenamiento y evalúe su desempeño con el conjunto de datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Según Luis Furlán, el Jupyter Notebook cuenta como el informe en sí.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
